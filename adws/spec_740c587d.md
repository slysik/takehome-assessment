[
  {
    "type": "system",
    "subtype": "init",
    "cwd": "/Users/slysik/tac/steve/adws",
    "session_id": "96c60ad3-afa6-42dc-8547-39a337fbaaca",
    "tools": [
      "Task",
      "Bash",
      "Glob",
      "Grep",
      "ExitPlanMode",
      "Read",
      "Edit",
      "Write",
      "NotebookEdit",
      "WebFetch",
      "TodoWrite",
      "WebSearch",
      "BashOutput",
      "KillShell",
      "Skill",
      "SlashCommand",
      "mcp__context7__resolve-library-id",
      "mcp__context7__get-library-docs",
      "mcp__firecrawl__firecrawl_scrape",
      "mcp__firecrawl__firecrawl_map",
      "mcp__firecrawl__firecrawl_search",
      "mcp__firecrawl__firecrawl_crawl",
      "mcp__firecrawl__firecrawl_check_crawl_status",
      "mcp__firecrawl__firecrawl_extract",
      "mcp__sequential-thinking__sequentialthinking",
      "mcp__playwright__start_codegen_session",
      "mcp__playwright__end_codegen_session",
      "mcp__playwright__get_codegen_session",
      "mcp__playwright__clear_codegen_session",
      "mcp__playwright__playwright_navigate",
      "mcp__playwright__playwright_screenshot",
      "mcp__playwright__playwright_click",
      "mcp__playwright__playwright_iframe_click",
      "mcp__playwright__playwright_iframe_fill",
      "mcp__playwright__playwright_fill",
      "mcp__playwright__playwright_select",
      "mcp__playwright__playwright_hover",
      "mcp__playwright__playwright_upload_file",
      "mcp__playwright__playwright_evaluate",
      "mcp__playwright__playwright_console_logs",
      "mcp__playwright__playwright_close",
      "mcp__playwright__playwright_get",
      "mcp__playwright__playwright_post",
      "mcp__playwright__playwright_put",
      "mcp__playwright__playwright_patch",
      "mcp__playwright__playwright_delete",
      "mcp__playwright__playwright_expect_response",
      "mcp__playwright__playwright_assert_response",
      "mcp__playwright__playwright_custom_user_agent",
      "mcp__playwright__playwright_get_visible_text",
      "mcp__playwright__playwright_get_visible_html",
      "mcp__playwright__playwright_go_back",
      "mcp__playwright__playwright_go_forward",
      "mcp__playwright__playwright_drag",
      "mcp__playwright__playwright_press_key",
      "mcp__playwright__playwright_save_as_pdf",
      "mcp__playwright__playwright_click_and_switch_tab",
      "ListMcpResourcesTool",
      "ReadMcpResourceTool",
      "mcp__reddit__fetch_reddit_hot_threads",
      "mcp__reddit__fetch_reddit_post_content",
      "mcp__fetch__fetch"
    ],
    "mcp_servers": [
      {
        "name": "elevenlabs",
        "status": "failed"
      },
      {
        "name": "context7",
        "status": "connected"
      },
      {
        "name": "firecrawl",
        "status": "connected"
      },
      {
        "name": "sequential-thinking",
        "status": "connected"
      },
      {
        "name": "playwright",
        "status": "connected"
      },
      {
        "name": "reddit",
        "status": "connected"
      },
      {
        "name": "fetch",
        "status": "connected"
      }
    ],
    "model": "claude-sonnet-4-5-20250929",
    "permissionMode": "default",
    "slash_commands": [
      "all_tools",
      "sentient",
      "question",
      "prime",
      "prime_tts",
      "crypto_research_haiku",
      "update_status_line",
      "cook",
      "crypto_research",
      "cook_research_only",
      "git_status",
      "classify_adw",
      "resolve_failed_e2e_test",
      "install",
      "test_e2e",
      "prepare_app",
      "generate_branch_name",
      "implement",
      "document",
      "chore",
      "pull_request",
      "bug",
      "patch",
      "conditional_docs",
      "resolve_failed_test",
      "start",
      "review",
      "commit",
      "prime",
      "test",
      "feature",
      "classify_issue",
      "tools",
      "e2e:test_csv_export",
      "e2e:test_sql_injection",
      "e2e:test_disable_input_debounce",
      "e2e:test_complex_query",
      "e2e:test_basic_query",
      "e2e:test_random_query_generator",
      "compact",
      "context",
      "cost",
      "init",
      "pr-comments",
      "release-notes",
      "todos",
      "review",
      "security-review",
      "mcp__fetch__fetch"
    ],
    "apiKeySource": "ANTHROPIC_API_KEY",
    "claude_code_version": "2.0.47",
    "output_style": "default",
    "agents": [
      "general-purpose",
      "statusline-setup",
      "Explore",
      "Plan",
      "documentation-writer",
      "security-auditor",
      "test-engineer"
    ],
    "skills": [],
    "plugins": [],
    "uuid": "f4c7d729-0dfb-40dc-bd93-76d3ed34d74c"
  },
  {
    "type": "assistant",
    "message": {
      "model": "claude-sonnet-4-5-20250929",
      "id": "msg_01JygVXstT9ow5pkQHWWJ8cf",
      "type": "message",
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "# Implementation Specification: Natural Language SQL Interface\n\n## 1. Project Overview\n\n### 1.1 Project Title\n**Natural Language SQL Interface (NLSI)** - An AI-powered web application for natural language to SQL query conversion with integrated data upload and visualization capabilities.\n\n### 1.2 Purpose\nEnable non-technical users and data analysts to query uploaded CSV/JSON datasets using natural language without SQL knowledge, while providing SQL transparency and injection protection for technical users.\n\n### 1.3 Key Features\n- Natural language to SQL conversion via OpenAI/Anthropic LLM\n- Multi-file drag-and-drop upload (.csv, .json, .jsonl)\n- Real-time SQL generation and execution\n- Interactive tabular result visualization\n- Database schema inspection and column insights\n- Multi-layered SQL injection protection\n- Query history tracking\n- CSV export functionality\n- Keyboard shortcuts (Cmd/Ctrl+Enter to execute)\n\n### 1.4 Target Users\n- **Primary**: Business analysts, data scientists, product managers\n- **Secondary**: Developers prototyping queries, QA testers validating data\n- **Use Cases**: Ad-hoc data analysis, report generation, data exploration, quick insights\n\n## 2. Architecture & Design\n\n### 2.1 System Components\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Frontend (Vite/TS)                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  UI Components                           \u2502   \u2502\n\u2502  \u2502  - Upload Modal                          \u2502   \u2502\n\u2502  \u2502  - Query Input                           \u2502   \u2502\n\u2502  \u2502  - Results Table                         \u2502   \u2502\n\u2502  \u2502  - Export Button                         \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  API Client (llm_client.py)             \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2195 HTTP/REST\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Backend (FastAPI)                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  API Endpoints (server.py)               \u2502   \u2502\n\u2502  \u2502  /api/upload, /api/query, /api/schema    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Core Modules                            \u2502   \u2502\n\u2502  \u2502  - file_processor.py                     \u2502   \u2502\n\u2502  \u2502  - llm_processor.py                      \u2502   \u2502\n\u2502  \u2502  - sql_processor.py                      \u2502   \u2502\n\u2502  \u2502  - sql_security.py                       \u2502   \u2502\n\u2502  \u2502  - insights.py                           \u2502   \u2502\n\u2502  \u2502  - csv_exporter.py                       \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  SQLite Database (in-memory/file)        \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 2.2 Technology Stack\n\n**Frontend**\n- **Runtime**: Bun (fast JavaScript runtime)\n- **Build Tool**: Vite 5.x (for HMR and optimized builds)\n- **Language**: TypeScript 5.x\n- **HTTP Client**: Fetch API\n\n**Backend**\n- **Framework**: FastAPI 0.100+\n- **Language**: Python 3.10+\n- **Package Manager**: uv (fast pip alternative)\n- **Database**: SQLite3 (via Python stdlib)\n- **LLM Integration**: OpenAI API, Anthropic API\n- **Environment**: python-dotenv for config\n\n**DevOps**\n- **Process Management**: Shell scripts (start.sh, stop_apps.sh)\n- **Testing**: pytest for backend, manual for frontend\n- **Version Control**: Git with semantic commits\n\n### 2.3 Design Patterns\n\n**Backend Patterns**\n- **Service Layer**: Core business logic in `core/` modules\n- **Dependency Injection**: FastAPI's built-in DI for database connections\n- **Repository Pattern**: SQL operations centralized in `sql_processor.py`\n- **Strategy Pattern**: LLM provider selection (OpenAI/Anthropic)\n- **Facade Pattern**: `sql_security.py` wraps all SQL operations\n\n**Frontend Patterns**\n- **Module Pattern**: Isolated API client\n- **Event-Driven**: Keyboard shortcuts, drag-and-drop handlers\n- **Component-Based**: Reusable UI modules\n\n**Security Patterns**\n- **Defense in Depth**: Multiple validation layers\n- **Whitelist Validation**: Identifier name checking\n- **Parameterized Queries**: No string concatenation\n- **Input Sanitization**: File name cleaning\n\n### 2.4 Data Models\n\n**Database Schema (Dynamic)**\n```sql\n-- Tables created from uploaded files\nCREATE TABLE {sanitized_filename} (\n    {column1} {inferred_type},\n    {column2} {inferred_type},\n    ...\n);\n```\n\n**API Request/Response Models**\n```python\n# Upload Request\nclass UploadRequest(BaseModel):\n    filename: str\n    content: str  # Base64 or raw content\n    \n# Query Request\nclass QueryRequest(BaseModel):\n    query: str  # Natural language\n    \n# Query Response\nclass QueryResponse(BaseModel):\n    sql: str\n    results: List[Dict[str, Any]]\n    columns: List[str]\n    row_count: int\n    \n# Schema Response\nclass SchemaResponse(BaseModel):\n    tables: Dict[str, List[str]]  # table_name -> [columns]\n```\n\n## 3. Implementation Requirements\n\n### 3.1 Core Features with Acceptance Criteria\n\n#### Feature 1: File Upload System\n**User Story**: As a user, I can upload CSV/JSON/JSONL files to query them.\n\n**Acceptance Criteria**:\n- [ ] Drag-and-drop interface accepts .csv, .json, .jsonl files\n- [ ] Sample data buttons load pre-configured datasets\n- [ ] File size limit: 50MB per file\n- [ ] Progress indicator during upload\n- [ ] Duplicate filename overwrites existing table with confirmation\n- [ ] Column names sanitized (alphanumeric + underscore)\n- [ ] Type inference: INTEGER, REAL, TEXT, NULL\n- [ ] Error messages for malformed files\n\n**Technical Requirements**:\n- Parse CSV with Python `csv` module\n- Parse JSON/JSONL with `json` module\n- Create SQLite tables with inferred schema\n- Store table metadata in memory\n\n#### Feature 2: Natural Language Query Processing\n**User Story**: As a user, I can ask questions in plain English and get SQL results.\n\n**Acceptance Criteria**:\n- [ ] Text input with Cmd/Ctrl+Enter shortcut\n- [ ] Debounced input (500ms) to prevent excessive API calls\n- [ ] Display generated SQL before execution\n- [ ] Show results in paginated table (50 rows/page)\n- [ ] Handle queries with no results gracefully\n- [ ] Support JOIN, WHERE, GROUP BY, ORDER BY, LIMIT\n- [ ] Prevent destructive operations (DELETE, DROP, UPDATE)\n- [ ] Display execution time\n\n**Technical Requirements**:\n- Send query + schema context to LLM\n- Parse LLM response for SQL statement\n- Validate SQL with `sql_security.validate_identifier()`\n- Execute with `sql_security.execute_query_safely()`\n- Return JSON-serializable results\n\n#### Feature 3: SQL Injection Protection\n**User Story**: As a developer, I trust the system prevents SQL injection attacks.\n\n**Acceptance Criteria**:\n- [ ] All identifiers validated with regex `^[a-zA-Z_][a-zA-Z0-9_]*$`\n- [ ] SQL keywords blocked as table/column names\n- [ ] No string concatenation in SQL generation\n- [ ] Parameterized queries for all user input\n- [ ] Multiple statements blocked (no `;` chaining)\n- [ ] SQL comments stripped (`--`, `/**/`)\n- [ ] Comprehensive test suite covers 20+ injection vectors\n\n**Technical Requirements**:\n- Centralized validation in `sql_security.py`\n- Use SQLite's `[identifier]` escaping for DDL\n- Use `?` placeholders for DML\n- Logging of blocked attempts\n\n#### Feature 4: Database Schema Management\n**User Story**: As a user, I can view uploaded tables and delete them.\n\n**Acceptance Criteria**:\n- [ ] List all tables with row counts\n- [ ] Click \"\u00d7\" button to delete table\n- [ ] Confirmation dialog for destructive actions\n- [ ] Refresh schema after upload/delete\n- [ ] Display column names and types\n\n**Technical Requirements**:\n- Query `sqlite_master` for table list\n- Use `PRAGMA table_info()` for column metadata\n- Safe deletion with validated identifiers\n\n#### Feature 5: CSV Export\n**User Story**: As a user, I can export query results to CSV.\n\n**Acceptance Criteria**:\n- [ ] \"Export\" button next to query results\n- [ ] Generates valid CSV with headers\n- [ ] Downloads with descriptive filename `query_results_YYYYMMDD_HHMMSS.csv`\n- [ ] Handles special characters (quotes, commas, newlines)\n- [ ] Exports all rows (not just visible page)\n\n**Technical Requirements**:\n- Use Python `csv` module with `QUOTE_MINIMAL`\n- Stream large result sets to avoid memory issues\n- Return as downloadable attachment\n\n### 3.2 Dependencies and Prerequisites\n\n**System Requirements**\n- Python 3.10+ with pip/uv\n- Node.js 18+ with npm/yarn/bun\n- SQLite 3.x (included in Python)\n- 2GB RAM minimum\n- 500MB disk space\n\n**External APIs**\n- OpenAI API Key (GPT-4 or GPT-3.5-turbo)\n- OR Anthropic API Key (Claude 3.5 Sonnet)\n- Internet connection for LLM calls\n\n**Development Tools**\n- Git 2.30+\n- Code editor (VS Code recommended)\n- pytest for testing\n- curl/Postman for API testing\n\n### 3.3 Integration Points\n\n**Frontend \u2194 Backend**\n- REST API over HTTP\n- JSON request/response bodies\n- CORS enabled for `http://localhost:5173`\n- Error responses use HTTP status codes\n\n**Backend \u2194 LLM Providers**\n- OpenAI API: `https://api.openai.com/v1/chat/completions`\n- Anthropic API: `https://api.anthropic.com/v1/messages`\n- Retry logic: 3 attempts with exponential backoff\n- Timeout: 30 seconds per request\n\n**Backend \u2194 SQLite**\n- In-memory database for development\n- File-based database for persistence\n- Connection pooling not needed (single-threaded)\n\n### 3.4 API Specifications\n\n**POST /api/upload**\n```http\nRequest:\nContent-Type: multipart/form-data\nBody: file (binary)\n\nResponse: 200 OK\n{\n  \"message\": \"File uploaded successfully\",\n  \"table_name\": \"users\",\n  \"row_count\": 150,\n  \"columns\": [\"id\", \"name\", \"email\", \"created_at\"]\n}\n```\n\n**POST /api/query**\n```http\nRequest:\nContent-Type: application/json\n{\n  \"query\": \"Show me users who signed up last week\"\n}\n\nResponse: 200 OK\n{\n  \"sql\": \"SELECT * FROM users WHERE created_at >= date('now', '-7 days')\",\n  \"results\": [\n    {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\", \"created_at\": \"2025-01-13\"},\n    ...\n  ],\n  \"columns\": [\"id\", \"name\", \"email\", \"created_at\"],\n  \"row_count\": 23,\n  \"execution_time_ms\": 15\n}\n```\n\n**GET /api/schema**\n```http\nResponse: 200 OK\n{\n  \"tables\": {\n    \"users\": [\"id\", \"name\", \"email\", \"created_at\"],\n    \"orders\": [\"id\", \"user_id\", \"total\", \"status\"]\n  }\n}\n```\n\n**POST /api/insights**\n```http\nRequest:\nContent-Type: application/json\n{\n  \"table_name\": \"users\"\n}\n\nResponse: 200 OK\n{\n  \"table\": \"users\",\n  \"insights\": {\n    \"id\": \"Primary key, integer, range: 1-150\",\n    \"email\": \"Unique emails, 98% valid format\",\n    \"created_at\": \"Dates from 2024-01-01 to 2025-01-20\"\n  }\n}\n```\n\n**DELETE /api/table/{table_name}**\n```http\nResponse: 200 OK\n{\n  \"message\": \"Table 'users' deleted successfully\"\n}\n```\n\n**GET /api/health**\n```http\nResponse: 200 OK\n{\n  \"status\": \"healthy\",\n  \"database\": \"connected\",\n  \"llm_provider\": \"openai\"\n}\n```\n\n## 4. Development Phases\n\n### Phase 1: Foundation (Week 1)\n**Goal**: Set up project structure and basic infrastructure.\n\n**Tasks**:\n1. Initialize Git repository with `.gitignore`\n2. Set up backend with FastAPI boilerplate\n3. Configure uv for dependency management\n4. Set up frontend with Vite + TypeScript\n5. Create `.env.sample` and environment loading\n6. Implement `/api/health` endpoint\n7. Configure CORS middleware\n8. Write project README\n\n**Deliverables**:\n- Working dev servers (frontend + backend)\n- Environment configuration documented\n- Health check endpoint functional\n\n### Phase 2: Core Features (Week 2-3)\n**Goal**: Implement file upload, NL query, and SQL execution.\n\n**Tasks**:\n1. Implement CSV parser in `file_processor.py`\n2. Implement JSON/JSONL parser\n3. Build table creation logic with type inference\n4. Create `/api/upload` endpoint\n5. Build frontend upload modal with drag-and-drop\n6. Implement LLM integration in `llm_processor.py`\n7. Create prompt engineering for SQL generation\n8. Build `/api/query` endpoint\n9. Implement frontend query input and results table\n10. Add keyboard shortcuts (Cmd/Ctrl+Enter)\n\n**Deliverables**:\n- Users can upload CSV/JSON files\n- Users can query data with natural language\n- Results display in browser\n\n### Phase 3: Security & Schema (Week 4)\n**Goal**: Implement SQL injection protection and schema management.\n\n**Tasks**:\n1. Create `sql_security.py` module\n2. Implement identifier validation\n3. Build safe query execution wrapper\n4. Add dangerous operation detection\n5. Refactor all SQL operations to use security module\n6. Implement `/api/schema` endpoint\n7. Build schema display in frontend\n8. Add table deletion functionality\n9. Implement confirmation dialogs\n\n**Deliverables**:\n- All SQL operations secured\n- Users can view and manage tables\n- Security test suite passes\n\n### Phase 4: Testing & Refinement (Week 5)\n**Goal**: Ensure reliability and handle edge cases.\n\n**Tasks**:\n1. Write unit tests for `file_processor.py`\n2. Write unit tests for `llm_processor.py`\n3. Write unit tests for `sql_processor.py`\n4. Write comprehensive SQL injection tests\n5. Add integration tests for API endpoints\n6. Test with large files (10MB+)\n7. Test with malformed data\n8. Add error handling and user-friendly messages\n9. Implement logging with structured logs\n10. Add column insights generation\n\n**Deliverables**:\n- 80%+ test coverage\n- All edge cases handled\n- Clear error messages\n\n### Phase 5: Polish & Documentation (Week 6)\n**Goal**: Prepare for production deployment.\n\n**Tasks**:\n1. Implement CSV export functionality\n2. Add query result pagination\n3. Optimize LLM prompts for accuracy\n4. Add loading states and spinners\n5. Implement query history (optional)\n6. Write API documentation\n7. Create user guide with screenshots\n8. Document security best practices\n9. Add troubleshooting guide\n10. Create demo video\n\n**Deliverables**:\n- Production-ready application\n- Comprehensive documentation\n- Demo materials\n\n## 5. Testing Strategy\n\n### 5.1 Unit Testing Approach\n\n**Backend Unit Tests** (`pytest`)\n```python\n# tests/core/test_file_processor.py\ndef test_csv_parsing_valid_file():\n    \"\"\"Test CSV file is parsed correctly\"\"\"\n    \ndef test_csv_parsing_empty_file():\n    \"\"\"Test empty CSV raises appropriate error\"\"\"\n    \ndef test_json_parsing_valid_array():\n    \"\"\"Test JSON array is converted to rows\"\"\"\n    \ndef test_jsonl_parsing_multiline():\n    \"\"\"Test JSONL with multiple objects\"\"\"\n    \ndef test_type_inference_integers():\n    \"\"\"Test column type inference for integers\"\"\"\n\n# tests/core/test_sql_security.py\ndef test_validate_identifier_valid():\n    \"\"\"Test valid identifiers pass validation\"\"\"\n    \ndef test_validate_identifier_sql_injection():\n    \"\"\"Test SQL injection attempts are blocked\"\"\"\n    \ndef test_execute_query_safely_select():\n    \"\"\"Test safe SELECT execution\"\"\"\n    \ndef test_execute_query_safely_blocks_drop():\n    \"\"\"Test DROP statement is blocked\"\"\"\n\n# tests/core/test_llm_processor.py\ndef test_llm_generates_valid_sql():\n    \"\"\"Test LLM returns parseable SQL\"\"\"\n    \ndef test_llm_handles_ambiguous_query():\n    \"\"\"Test LLM requests clarification\"\"\"\n```\n\n**Coverage Target**: 80% line coverage, 100% for `sql_security.py`\n\n### 5.2 Integration Testing Requirements\n\n**API Integration Tests**\n```python\n# tests/test_api.py\ndef test_upload_csv_creates_table():\n    \"\"\"Test POST /api/upload creates table\"\"\"\n    \ndef test_query_returns_results():\n    \"\"\"Test POST /api/query returns data\"\"\"\n    \ndef test_schema_lists_tables():\n    \"\"\"Test GET /api/schema returns table list\"\"\"\n    \ndef test_delete_table_removes_data():\n    \"\"\"Test DELETE /api/table/{name} works\"\"\"\n```\n\n**LLM Integration Tests**\n- Mock LLM responses for consistent testing\n- Test with real API calls in CI/CD (optional)\n- Validate SQL syntax of generated queries\n\n### 5.3 E2E/Acceptance Test Scenarios\n\n**Manual Test Cases**\n1. **Happy Path**: Upload CSV \u2192 Query \u2192 View Results \u2192 Export\n2. **File Overwrite**: Upload same filename twice, confirm overwrite\n3. **Complex Query**: Test JOIN between two tables\n4. **Error Handling**: Upload malformed CSV, verify error message\n5. **SQL Injection**: Attempt injection, verify blocking\n6. **Large File**: Upload 10MB file, verify performance\n7. **Mobile Layout**: Test on mobile browser (responsive design)\n\n**Automated E2E** (Playwright/Cypress - optional)\n- Simulate full user workflow\n- Test keyboard shortcuts\n- Validate drag-and-drop\n\n### 5.4 Performance/Security Testing Needs\n\n**Performance Benchmarks**\n- Upload 1MB file: < 2 seconds\n- Query execution: < 1 second (excluding LLM)\n- LLM response time: < 5 seconds\n- Frontend initial load: < 1 second\n- Memory usage: < 500MB for 50MB database\n\n**Security Testing**\n- Run OWASP SQL injection test vectors\n- Test file upload with malicious filenames\n- Verify CORS headers prevent unauthorized access\n- Check for sensitive data in error messages\n- Test rate limiting (if implemented)\n\n**Load Testing** (optional)\n- 100 concurrent users with `locust`\n- Measure throughput (requests/second)\n- Identify memory leaks with prolonged usage\n\n## 6. Success Criteria\n\n### 6.1 Definition of Done\n\nA feature is \"done\" when:\n- [ ] Code is written and reviewed\n- [ ] Unit tests pass (80%+ coverage)\n- [ ] Integration tests pass\n- [ ] Manual testing completed\n- [ ] Documentation updated\n- [ ] No critical bugs\n- [ ] Deployed to staging environment\n\n### 6.2 Performance Benchmarks\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| File upload (1MB) | < 2s | `time curl` |\n| Query execution | < 1s | Backend logs |\n| LLM response | < 5s | API monitoring |\n| Frontend load | < 1s | Lighthouse score |\n| Memory usage | < 500MB | `ps aux` |\n\n### 6.3 Quality Metrics\n\n- **Code Coverage**: \u2265 80% overall, 100% for security module\n- **Bug Rate**: < 5 critical bugs in production/month\n- **API Uptime**: 99.5%\n- **User Satisfaction**: 4+ stars on feedback (if applicable)\n- **Security Score**: 0 SQL injection vulnerabilities\n\n### 6.4 User Acceptance Criteria\n\n**Must Have**\n- Upload CSV and JSON files without errors\n- Query data using natural language\n- View results in readable table format\n- Delete uploaded tables\n- Export results to CSV\n\n**Should Have**\n- Query history to revisit past queries\n- Column insights for data exploration\n- Sample data for quick testing\n- Keyboard shortcuts for efficiency\n\n**Nice to Have**\n- Multiple LLM provider support\n- Query suggestions/autocomplete\n- Data visualization (charts)\n- Collaborative features (share queries)\n\n## 7. Known Risks & Mitigation\n\n### 7.1 Technical Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|-------------|--------|------------|\n| LLM generates invalid SQL | High | High | Validate SQL syntax before execution; retry with error feedback |\n| SQL injection bypass | Low | Critical | Multiple validation layers; comprehensive testing; security audit |\n| Large file upload crashes server | Medium | Medium | Implement file size limits; stream processing; progress indicators |\n| LLM API rate limits | Medium | High | Implement caching; retry logic; fallback to alternative provider |\n| Database performance degrades | Low | Medium | Use indexes; query optimization; pagination |\n\n### 7.2 Scheduling Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|-------------|--------|------------|\n| LLM integration takes longer than expected | Medium | High | Start with single provider (OpenAI); add Anthropic later |\n| Frontend complexity increases scope | Medium | Medium | Use existing UI libraries; defer nice-to-have features |\n| Testing phase reveals major bugs | High | Medium | Allocate buffer time; prioritize critical bugs |\n\n### 7.3 Mitigation Strategies\n\n**For Invalid SQL Generation**:\n1. Use structured prompts with examples\n2. Parse LLM response for SQL code blocks\n3. Validate syntax with `sqlite3.parse()`\n4. Provide schema context in every request\n5. Log failures for prompt tuning\n\n**For Rate Limiting**:\n1. Implement client-side caching (cache LLM responses)\n2. Add retry logic with exponential backoff\n3. Display rate limit warnings to users\n4. Support both OpenAI and Anthropic for redundancy\n\n**For Performance Issues**:\n1. Profile slow queries with `EXPLAIN QUERY PLAN`\n2. Add indexes on frequently queried columns\n3. Implement pagination (LIMIT/OFFSET)\n4. Use in-memory database for small datasets\n\n## 8. References\n\n### 8.1 Documentation\n\n**Framework Documentation**\n- FastAPI: https://fastapi.tiangolo.com/\n- Vite: https://vitejs.dev/\n- TypeScript: https://www.typescriptlang.org/docs/\n- SQLite: https://www.sqlite.org/docs.html\n- pytest: https://docs.pytest.org/\n\n**LLM API Documentation**\n- OpenAI API: https://platform.openai.com/docs/api-reference\n- Anthropic API: https://docs.anthropic.com/claude/reference/\n\n**Security Resources**\n- OWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection\n- SQLite Security: https://www.sqlite.org/security.html\n\n### 8.2 External Resources\n\n**Tools**\n- uv (Python package manager): https://github.com/astral-sh/uv\n- Bun (JavaScript runtime): https://bun.sh/\n- GitHub CLI: https://cli.github.com/\n\n**Libraries**\n- python-dotenv: https://pypi.org/project/python-dotenv/\n- Starlette (FastAPI base): https://www.starlette.io/\n\n### 8.3 Related Projects\n\n**Similar Open-Source Projects**\n- Text-to-SQL: https://github.com/sqlchat/sqlchat\n- DataChat: https://github.com/datachat/datachat\n- Vanna.AI: https://github.com/vanna-ai/vanna\n\n**ADW System**\n- Internal automation framework for GitHub issue processing\n- Located in `adws/` directory\n- Uses Claude Code CLI for autonomous implementation\n- Separate from main application\n\n---\n\n## Appendix A: Directory Structure\n\n```\n.\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 client/                 # Frontend (Vite + TypeScript)\n\u2502   \u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.py         # Entry point (CORRECTED: should be main.ts)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_client.py   # API client\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow/       # UI workflows\n\u2502   \u2502   \u251c\u2500\u2500 data/               # Sample datasets\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 uv.lock\n\u2502   \u2502   \u2514\u2500\u2500 run.sh\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 server/                 # Backend (FastAPI)\n\u2502       \u251c\u2500\u2500 core/\n\u2502       \u2502   \u251c\u2500\u2500 file_processor.py    # CSV/JSON parsing\n\u2502       \u2502   \u251c\u2500\u2500 llm_processor.py     # LLM integration\n\u2502       \u2502   \u251c\u2500\u2500 sql_processor.py     # SQL execution\n\u2502       \u2502   \u251c\u2500\u2500 sql_security.py      # Security module\n\u2502       \u2502   \u251c\u2500\u2500 insights.py          # Column insights\n\u2502       \u2502   \u251c\u2500\u2500 csv_exporter.py      # Export functionality\n\u2502       \u2502   \u251c\u2500\u2500 data_models.py       # Pydantic models\n\u2502       \u2502   \u2514\u2500\u2500 constants.py         # Config constants\n\u2502       \u251c\u2500\u2500 tests/\n\u2502       \u2502   \u251c\u2500\u2500 core/\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 test_file_processor.py\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 test_llm_processor.py\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 test_sql_processor.py\n\u2502       \u2502   \u251c\u2500\u2500 assets/          # Test fixtures\n\u2502       \u2502   \u2514\u2500\u2500 test_sql_injection.py\n\u2502       \u251c\u2500\u2500 server.py            # FastAPI app\n\u2502       \u251c\u2500\u2500 main.py              # Entry point\n\u2502       \u251c\u2500\u2500 pyproject.toml       # Dependencies\n\u2502       \u251c\u2500\u2500 uv.lock\n\u2502       \u2514\u2500\u2500 .env.sample\n\u2502\n\u251c\u2500\u2500 adws/                        # ADW automation system\n\u2502   \u251c\u2500\u2500 adw_plan_build.py\n\u2502   \u251c\u2500\u2500 trigger_cron.py\n\u2502   \u251c\u2500\u2500 trigger_webhook.py\n\u2502   \u2514\u2500\u2500 README.md\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 start.sh                 # Start both services\n\u2502   \u251c\u2500\u2500 stop_apps.sh             # Stop all services\n\u2502   \u251c\u2500\u2500 reset_db.sh              # Reset database\n\u2502   \u2514\u2500\u2500 copy_dot_env.sh\n\u2502\n\u251c\u2500\u2500 specs/                       # Feature specifications\n\u251c\u2500\u2500 app_docs/                    # Feature documentation\n\u251c\u2500\u2500 agents/                      # Agent execution logs\n\u251c\u2500\u2500 logs/                        # Structured logs\n\u2514\u2500\u2500 README.md                    # Project README\n```\n\n## Appendix B: Environment Variables\n\n```bash\n# .env.sample (app/server/)\n\n# LLM Provider (choose one or both)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n\n# LLM Configuration\nLLM_PROVIDER=openai              # or 'anthropic'\nLLM_MODEL=gpt-4                  # or 'claude-3-5-sonnet-20241022'\nLLM_TEMPERATURE=0.0              # Lower = more deterministic\n\n# Database\nDATABASE_PATH=./db/app.db        # or ':memory:' for in-memory\nDATABASE_BACKUP_PATH=./db/backup.db\n\n# Server\nHOST=0.0.0.0\nPORT=8000\nDEBUG=true\n\n# CORS\nCORS_ORIGINS=http://localhost:5173,http://localhost:3000\n\n# File Upload\nMAX_FILE_SIZE_MB=50\nALLOWED_EXTENSIONS=.csv,.json,.jsonl\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=./logs/app.log\n```\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: 2025-01-20  \n**Author**: AI Specification Generator  \n**Status**: Ready for Implementation"
        }
      ],
      "stop_reason": "end_turn",
      "stop_sequence": null,
      "usage": {
        "input_tokens": 2,
        "cache_creation_input_tokens": 33765,
        "cache_read_input_tokens": 0,
        "cache_creation": {
          "ephemeral_5m_input_tokens": 33765,
          "ephemeral_1h_input_tokens": 0
        },
        "output_tokens": 7228,
        "service_tier": "standard"
      },
      "context_management": null
    },
    "parent_tool_use_id": null,
    "session_id": "96c60ad3-afa6-42dc-8547-39a337fbaaca",
    "uuid": "694b7e5d-f1cd-4e63-a33f-6e491fb7820f"
  },
  {
    "type": "result",
    "subtype": "success",
    "is_error": false,
    "duration_ms": 122652,
    "duration_api_ms": 129788,
    "num_turns": 1,
    "result": "# Implementation Specification: Natural Language SQL Interface\n\n## 1. Project Overview\n\n### 1.1 Project Title\n**Natural Language SQL Interface (NLSI)** - An AI-powered web application for natural language to SQL query conversion with integrated data upload and visualization capabilities.\n\n### 1.2 Purpose\nEnable non-technical users and data analysts to query uploaded CSV/JSON datasets using natural language without SQL knowledge, while providing SQL transparency and injection protection for technical users.\n\n### 1.3 Key Features\n- Natural language to SQL conversion via OpenAI/Anthropic LLM\n- Multi-file drag-and-drop upload (.csv, .json, .jsonl)\n- Real-time SQL generation and execution\n- Interactive tabular result visualization\n- Database schema inspection and column insights\n- Multi-layered SQL injection protection\n- Query history tracking\n- CSV export functionality\n- Keyboard shortcuts (Cmd/Ctrl+Enter to execute)\n\n### 1.4 Target Users\n- **Primary**: Business analysts, data scientists, product managers\n- **Secondary**: Developers prototyping queries, QA testers validating data\n- **Use Cases**: Ad-hoc data analysis, report generation, data exploration, quick insights\n\n## 2. Architecture & Design\n\n### 2.1 System Components\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Frontend (Vite/TS)                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  UI Components                           \u2502   \u2502\n\u2502  \u2502  - Upload Modal                          \u2502   \u2502\n\u2502  \u2502  - Query Input                           \u2502   \u2502\n\u2502  \u2502  - Results Table                         \u2502   \u2502\n\u2502  \u2502  - Export Button                         \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  API Client (llm_client.py)             \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2195 HTTP/REST\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Backend (FastAPI)                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  API Endpoints (server.py)               \u2502   \u2502\n\u2502  \u2502  /api/upload, /api/query, /api/schema    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Core Modules                            \u2502   \u2502\n\u2502  \u2502  - file_processor.py                     \u2502   \u2502\n\u2502  \u2502  - llm_processor.py                      \u2502   \u2502\n\u2502  \u2502  - sql_processor.py                      \u2502   \u2502\n\u2502  \u2502  - sql_security.py                       \u2502   \u2502\n\u2502  \u2502  - insights.py                           \u2502   \u2502\n\u2502  \u2502  - csv_exporter.py                       \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  SQLite Database (in-memory/file)        \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 2.2 Technology Stack\n\n**Frontend**\n- **Runtime**: Bun (fast JavaScript runtime)\n- **Build Tool**: Vite 5.x (for HMR and optimized builds)\n- **Language**: TypeScript 5.x\n- **HTTP Client**: Fetch API\n\n**Backend**\n- **Framework**: FastAPI 0.100+\n- **Language**: Python 3.10+\n- **Package Manager**: uv (fast pip alternative)\n- **Database**: SQLite3 (via Python stdlib)\n- **LLM Integration**: OpenAI API, Anthropic API\n- **Environment**: python-dotenv for config\n\n**DevOps**\n- **Process Management**: Shell scripts (start.sh, stop_apps.sh)\n- **Testing**: pytest for backend, manual for frontend\n- **Version Control**: Git with semantic commits\n\n### 2.3 Design Patterns\n\n**Backend Patterns**\n- **Service Layer**: Core business logic in `core/` modules\n- **Dependency Injection**: FastAPI's built-in DI for database connections\n- **Repository Pattern**: SQL operations centralized in `sql_processor.py`\n- **Strategy Pattern**: LLM provider selection (OpenAI/Anthropic)\n- **Facade Pattern**: `sql_security.py` wraps all SQL operations\n\n**Frontend Patterns**\n- **Module Pattern**: Isolated API client\n- **Event-Driven**: Keyboard shortcuts, drag-and-drop handlers\n- **Component-Based**: Reusable UI modules\n\n**Security Patterns**\n- **Defense in Depth**: Multiple validation layers\n- **Whitelist Validation**: Identifier name checking\n- **Parameterized Queries**: No string concatenation\n- **Input Sanitization**: File name cleaning\n\n### 2.4 Data Models\n\n**Database Schema (Dynamic)**\n```sql\n-- Tables created from uploaded files\nCREATE TABLE {sanitized_filename} (\n    {column1} {inferred_type},\n    {column2} {inferred_type},\n    ...\n);\n```\n\n**API Request/Response Models**\n```python\n# Upload Request\nclass UploadRequest(BaseModel):\n    filename: str\n    content: str  # Base64 or raw content\n    \n# Query Request\nclass QueryRequest(BaseModel):\n    query: str  # Natural language\n    \n# Query Response\nclass QueryResponse(BaseModel):\n    sql: str\n    results: List[Dict[str, Any]]\n    columns: List[str]\n    row_count: int\n    \n# Schema Response\nclass SchemaResponse(BaseModel):\n    tables: Dict[str, List[str]]  # table_name -> [columns]\n```\n\n## 3. Implementation Requirements\n\n### 3.1 Core Features with Acceptance Criteria\n\n#### Feature 1: File Upload System\n**User Story**: As a user, I can upload CSV/JSON/JSONL files to query them.\n\n**Acceptance Criteria**:\n- [ ] Drag-and-drop interface accepts .csv, .json, .jsonl files\n- [ ] Sample data buttons load pre-configured datasets\n- [ ] File size limit: 50MB per file\n- [ ] Progress indicator during upload\n- [ ] Duplicate filename overwrites existing table with confirmation\n- [ ] Column names sanitized (alphanumeric + underscore)\n- [ ] Type inference: INTEGER, REAL, TEXT, NULL\n- [ ] Error messages for malformed files\n\n**Technical Requirements**:\n- Parse CSV with Python `csv` module\n- Parse JSON/JSONL with `json` module\n- Create SQLite tables with inferred schema\n- Store table metadata in memory\n\n#### Feature 2: Natural Language Query Processing\n**User Story**: As a user, I can ask questions in plain English and get SQL results.\n\n**Acceptance Criteria**:\n- [ ] Text input with Cmd/Ctrl+Enter shortcut\n- [ ] Debounced input (500ms) to prevent excessive API calls\n- [ ] Display generated SQL before execution\n- [ ] Show results in paginated table (50 rows/page)\n- [ ] Handle queries with no results gracefully\n- [ ] Support JOIN, WHERE, GROUP BY, ORDER BY, LIMIT\n- [ ] Prevent destructive operations (DELETE, DROP, UPDATE)\n- [ ] Display execution time\n\n**Technical Requirements**:\n- Send query + schema context to LLM\n- Parse LLM response for SQL statement\n- Validate SQL with `sql_security.validate_identifier()`\n- Execute with `sql_security.execute_query_safely()`\n- Return JSON-serializable results\n\n#### Feature 3: SQL Injection Protection\n**User Story**: As a developer, I trust the system prevents SQL injection attacks.\n\n**Acceptance Criteria**:\n- [ ] All identifiers validated with regex `^[a-zA-Z_][a-zA-Z0-9_]*$`\n- [ ] SQL keywords blocked as table/column names\n- [ ] No string concatenation in SQL generation\n- [ ] Parameterized queries for all user input\n- [ ] Multiple statements blocked (no `;` chaining)\n- [ ] SQL comments stripped (`--`, `/**/`)\n- [ ] Comprehensive test suite covers 20+ injection vectors\n\n**Technical Requirements**:\n- Centralized validation in `sql_security.py`\n- Use SQLite's `[identifier]` escaping for DDL\n- Use `?` placeholders for DML\n- Logging of blocked attempts\n\n#### Feature 4: Database Schema Management\n**User Story**: As a user, I can view uploaded tables and delete them.\n\n**Acceptance Criteria**:\n- [ ] List all tables with row counts\n- [ ] Click \"\u00d7\" button to delete table\n- [ ] Confirmation dialog for destructive actions\n- [ ] Refresh schema after upload/delete\n- [ ] Display column names and types\n\n**Technical Requirements**:\n- Query `sqlite_master` for table list\n- Use `PRAGMA table_info()` for column metadata\n- Safe deletion with validated identifiers\n\n#### Feature 5: CSV Export\n**User Story**: As a user, I can export query results to CSV.\n\n**Acceptance Criteria**:\n- [ ] \"Export\" button next to query results\n- [ ] Generates valid CSV with headers\n- [ ] Downloads with descriptive filename `query_results_YYYYMMDD_HHMMSS.csv`\n- [ ] Handles special characters (quotes, commas, newlines)\n- [ ] Exports all rows (not just visible page)\n\n**Technical Requirements**:\n- Use Python `csv` module with `QUOTE_MINIMAL`\n- Stream large result sets to avoid memory issues\n- Return as downloadable attachment\n\n### 3.2 Dependencies and Prerequisites\n\n**System Requirements**\n- Python 3.10+ with pip/uv\n- Node.js 18+ with npm/yarn/bun\n- SQLite 3.x (included in Python)\n- 2GB RAM minimum\n- 500MB disk space\n\n**External APIs**\n- OpenAI API Key (GPT-4 or GPT-3.5-turbo)\n- OR Anthropic API Key (Claude 3.5 Sonnet)\n- Internet connection for LLM calls\n\n**Development Tools**\n- Git 2.30+\n- Code editor (VS Code recommended)\n- pytest for testing\n- curl/Postman for API testing\n\n### 3.3 Integration Points\n\n**Frontend \u2194 Backend**\n- REST API over HTTP\n- JSON request/response bodies\n- CORS enabled for `http://localhost:5173`\n- Error responses use HTTP status codes\n\n**Backend \u2194 LLM Providers**\n- OpenAI API: `https://api.openai.com/v1/chat/completions`\n- Anthropic API: `https://api.anthropic.com/v1/messages`\n- Retry logic: 3 attempts with exponential backoff\n- Timeout: 30 seconds per request\n\n**Backend \u2194 SQLite**\n- In-memory database for development\n- File-based database for persistence\n- Connection pooling not needed (single-threaded)\n\n### 3.4 API Specifications\n\n**POST /api/upload**\n```http\nRequest:\nContent-Type: multipart/form-data\nBody: file (binary)\n\nResponse: 200 OK\n{\n  \"message\": \"File uploaded successfully\",\n  \"table_name\": \"users\",\n  \"row_count\": 150,\n  \"columns\": [\"id\", \"name\", \"email\", \"created_at\"]\n}\n```\n\n**POST /api/query**\n```http\nRequest:\nContent-Type: application/json\n{\n  \"query\": \"Show me users who signed up last week\"\n}\n\nResponse: 200 OK\n{\n  \"sql\": \"SELECT * FROM users WHERE created_at >= date('now', '-7 days')\",\n  \"results\": [\n    {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\", \"created_at\": \"2025-01-13\"},\n    ...\n  ],\n  \"columns\": [\"id\", \"name\", \"email\", \"created_at\"],\n  \"row_count\": 23,\n  \"execution_time_ms\": 15\n}\n```\n\n**GET /api/schema**\n```http\nResponse: 200 OK\n{\n  \"tables\": {\n    \"users\": [\"id\", \"name\", \"email\", \"created_at\"],\n    \"orders\": [\"id\", \"user_id\", \"total\", \"status\"]\n  }\n}\n```\n\n**POST /api/insights**\n```http\nRequest:\nContent-Type: application/json\n{\n  \"table_name\": \"users\"\n}\n\nResponse: 200 OK\n{\n  \"table\": \"users\",\n  \"insights\": {\n    \"id\": \"Primary key, integer, range: 1-150\",\n    \"email\": \"Unique emails, 98% valid format\",\n    \"created_at\": \"Dates from 2024-01-01 to 2025-01-20\"\n  }\n}\n```\n\n**DELETE /api/table/{table_name}**\n```http\nResponse: 200 OK\n{\n  \"message\": \"Table 'users' deleted successfully\"\n}\n```\n\n**GET /api/health**\n```http\nResponse: 200 OK\n{\n  \"status\": \"healthy\",\n  \"database\": \"connected\",\n  \"llm_provider\": \"openai\"\n}\n```\n\n## 4. Development Phases\n\n### Phase 1: Foundation (Week 1)\n**Goal**: Set up project structure and basic infrastructure.\n\n**Tasks**:\n1. Initialize Git repository with `.gitignore`\n2. Set up backend with FastAPI boilerplate\n3. Configure uv for dependency management\n4. Set up frontend with Vite + TypeScript\n5. Create `.env.sample` and environment loading\n6. Implement `/api/health` endpoint\n7. Configure CORS middleware\n8. Write project README\n\n**Deliverables**:\n- Working dev servers (frontend + backend)\n- Environment configuration documented\n- Health check endpoint functional\n\n### Phase 2: Core Features (Week 2-3)\n**Goal**: Implement file upload, NL query, and SQL execution.\n\n**Tasks**:\n1. Implement CSV parser in `file_processor.py`\n2. Implement JSON/JSONL parser\n3. Build table creation logic with type inference\n4. Create `/api/upload` endpoint\n5. Build frontend upload modal with drag-and-drop\n6. Implement LLM integration in `llm_processor.py`\n7. Create prompt engineering for SQL generation\n8. Build `/api/query` endpoint\n9. Implement frontend query input and results table\n10. Add keyboard shortcuts (Cmd/Ctrl+Enter)\n\n**Deliverables**:\n- Users can upload CSV/JSON files\n- Users can query data with natural language\n- Results display in browser\n\n### Phase 3: Security & Schema (Week 4)\n**Goal**: Implement SQL injection protection and schema management.\n\n**Tasks**:\n1. Create `sql_security.py` module\n2. Implement identifier validation\n3. Build safe query execution wrapper\n4. Add dangerous operation detection\n5. Refactor all SQL operations to use security module\n6. Implement `/api/schema` endpoint\n7. Build schema display in frontend\n8. Add table deletion functionality\n9. Implement confirmation dialogs\n\n**Deliverables**:\n- All SQL operations secured\n- Users can view and manage tables\n- Security test suite passes\n\n### Phase 4: Testing & Refinement (Week 5)\n**Goal**: Ensure reliability and handle edge cases.\n\n**Tasks**:\n1. Write unit tests for `file_processor.py`\n2. Write unit tests for `llm_processor.py`\n3. Write unit tests for `sql_processor.py`\n4. Write comprehensive SQL injection tests\n5. Add integration tests for API endpoints\n6. Test with large files (10MB+)\n7. Test with malformed data\n8. Add error handling and user-friendly messages\n9. Implement logging with structured logs\n10. Add column insights generation\n\n**Deliverables**:\n- 80%+ test coverage\n- All edge cases handled\n- Clear error messages\n\n### Phase 5: Polish & Documentation (Week 6)\n**Goal**: Prepare for production deployment.\n\n**Tasks**:\n1. Implement CSV export functionality\n2. Add query result pagination\n3. Optimize LLM prompts for accuracy\n4. Add loading states and spinners\n5. Implement query history (optional)\n6. Write API documentation\n7. Create user guide with screenshots\n8. Document security best practices\n9. Add troubleshooting guide\n10. Create demo video\n\n**Deliverables**:\n- Production-ready application\n- Comprehensive documentation\n- Demo materials\n\n## 5. Testing Strategy\n\n### 5.1 Unit Testing Approach\n\n**Backend Unit Tests** (`pytest`)\n```python\n# tests/core/test_file_processor.py\ndef test_csv_parsing_valid_file():\n    \"\"\"Test CSV file is parsed correctly\"\"\"\n    \ndef test_csv_parsing_empty_file():\n    \"\"\"Test empty CSV raises appropriate error\"\"\"\n    \ndef test_json_parsing_valid_array():\n    \"\"\"Test JSON array is converted to rows\"\"\"\n    \ndef test_jsonl_parsing_multiline():\n    \"\"\"Test JSONL with multiple objects\"\"\"\n    \ndef test_type_inference_integers():\n    \"\"\"Test column type inference for integers\"\"\"\n\n# tests/core/test_sql_security.py\ndef test_validate_identifier_valid():\n    \"\"\"Test valid identifiers pass validation\"\"\"\n    \ndef test_validate_identifier_sql_injection():\n    \"\"\"Test SQL injection attempts are blocked\"\"\"\n    \ndef test_execute_query_safely_select():\n    \"\"\"Test safe SELECT execution\"\"\"\n    \ndef test_execute_query_safely_blocks_drop():\n    \"\"\"Test DROP statement is blocked\"\"\"\n\n# tests/core/test_llm_processor.py\ndef test_llm_generates_valid_sql():\n    \"\"\"Test LLM returns parseable SQL\"\"\"\n    \ndef test_llm_handles_ambiguous_query():\n    \"\"\"Test LLM requests clarification\"\"\"\n```\n\n**Coverage Target**: 80% line coverage, 100% for `sql_security.py`\n\n### 5.2 Integration Testing Requirements\n\n**API Integration Tests**\n```python\n# tests/test_api.py\ndef test_upload_csv_creates_table():\n    \"\"\"Test POST /api/upload creates table\"\"\"\n    \ndef test_query_returns_results():\n    \"\"\"Test POST /api/query returns data\"\"\"\n    \ndef test_schema_lists_tables():\n    \"\"\"Test GET /api/schema returns table list\"\"\"\n    \ndef test_delete_table_removes_data():\n    \"\"\"Test DELETE /api/table/{name} works\"\"\"\n```\n\n**LLM Integration Tests**\n- Mock LLM responses for consistent testing\n- Test with real API calls in CI/CD (optional)\n- Validate SQL syntax of generated queries\n\n### 5.3 E2E/Acceptance Test Scenarios\n\n**Manual Test Cases**\n1. **Happy Path**: Upload CSV \u2192 Query \u2192 View Results \u2192 Export\n2. **File Overwrite**: Upload same filename twice, confirm overwrite\n3. **Complex Query**: Test JOIN between two tables\n4. **Error Handling**: Upload malformed CSV, verify error message\n5. **SQL Injection**: Attempt injection, verify blocking\n6. **Large File**: Upload 10MB file, verify performance\n7. **Mobile Layout**: Test on mobile browser (responsive design)\n\n**Automated E2E** (Playwright/Cypress - optional)\n- Simulate full user workflow\n- Test keyboard shortcuts\n- Validate drag-and-drop\n\n### 5.4 Performance/Security Testing Needs\n\n**Performance Benchmarks**\n- Upload 1MB file: < 2 seconds\n- Query execution: < 1 second (excluding LLM)\n- LLM response time: < 5 seconds\n- Frontend initial load: < 1 second\n- Memory usage: < 500MB for 50MB database\n\n**Security Testing**\n- Run OWASP SQL injection test vectors\n- Test file upload with malicious filenames\n- Verify CORS headers prevent unauthorized access\n- Check for sensitive data in error messages\n- Test rate limiting (if implemented)\n\n**Load Testing** (optional)\n- 100 concurrent users with `locust`\n- Measure throughput (requests/second)\n- Identify memory leaks with prolonged usage\n\n## 6. Success Criteria\n\n### 6.1 Definition of Done\n\nA feature is \"done\" when:\n- [ ] Code is written and reviewed\n- [ ] Unit tests pass (80%+ coverage)\n- [ ] Integration tests pass\n- [ ] Manual testing completed\n- [ ] Documentation updated\n- [ ] No critical bugs\n- [ ] Deployed to staging environment\n\n### 6.2 Performance Benchmarks\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| File upload (1MB) | < 2s | `time curl` |\n| Query execution | < 1s | Backend logs |\n| LLM response | < 5s | API monitoring |\n| Frontend load | < 1s | Lighthouse score |\n| Memory usage | < 500MB | `ps aux` |\n\n### 6.3 Quality Metrics\n\n- **Code Coverage**: \u2265 80% overall, 100% for security module\n- **Bug Rate**: < 5 critical bugs in production/month\n- **API Uptime**: 99.5%\n- **User Satisfaction**: 4+ stars on feedback (if applicable)\n- **Security Score**: 0 SQL injection vulnerabilities\n\n### 6.4 User Acceptance Criteria\n\n**Must Have**\n- Upload CSV and JSON files without errors\n- Query data using natural language\n- View results in readable table format\n- Delete uploaded tables\n- Export results to CSV\n\n**Should Have**\n- Query history to revisit past queries\n- Column insights for data exploration\n- Sample data for quick testing\n- Keyboard shortcuts for efficiency\n\n**Nice to Have**\n- Multiple LLM provider support\n- Query suggestions/autocomplete\n- Data visualization (charts)\n- Collaborative features (share queries)\n\n## 7. Known Risks & Mitigation\n\n### 7.1 Technical Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|-------------|--------|------------|\n| LLM generates invalid SQL | High | High | Validate SQL syntax before execution; retry with error feedback |\n| SQL injection bypass | Low | Critical | Multiple validation layers; comprehensive testing; security audit |\n| Large file upload crashes server | Medium | Medium | Implement file size limits; stream processing; progress indicators |\n| LLM API rate limits | Medium | High | Implement caching; retry logic; fallback to alternative provider |\n| Database performance degrades | Low | Medium | Use indexes; query optimization; pagination |\n\n### 7.2 Scheduling Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|-------------|--------|------------|\n| LLM integration takes longer than expected | Medium | High | Start with single provider (OpenAI); add Anthropic later |\n| Frontend complexity increases scope | Medium | Medium | Use existing UI libraries; defer nice-to-have features |\n| Testing phase reveals major bugs | High | Medium | Allocate buffer time; prioritize critical bugs |\n\n### 7.3 Mitigation Strategies\n\n**For Invalid SQL Generation**:\n1. Use structured prompts with examples\n2. Parse LLM response for SQL code blocks\n3. Validate syntax with `sqlite3.parse()`\n4. Provide schema context in every request\n5. Log failures for prompt tuning\n\n**For Rate Limiting**:\n1. Implement client-side caching (cache LLM responses)\n2. Add retry logic with exponential backoff\n3. Display rate limit warnings to users\n4. Support both OpenAI and Anthropic for redundancy\n\n**For Performance Issues**:\n1. Profile slow queries with `EXPLAIN QUERY PLAN`\n2. Add indexes on frequently queried columns\n3. Implement pagination (LIMIT/OFFSET)\n4. Use in-memory database for small datasets\n\n## 8. References\n\n### 8.1 Documentation\n\n**Framework Documentation**\n- FastAPI: https://fastapi.tiangolo.com/\n- Vite: https://vitejs.dev/\n- TypeScript: https://www.typescriptlang.org/docs/\n- SQLite: https://www.sqlite.org/docs.html\n- pytest: https://docs.pytest.org/\n\n**LLM API Documentation**\n- OpenAI API: https://platform.openai.com/docs/api-reference\n- Anthropic API: https://docs.anthropic.com/claude/reference/\n\n**Security Resources**\n- OWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection\n- SQLite Security: https://www.sqlite.org/security.html\n\n### 8.2 External Resources\n\n**Tools**\n- uv (Python package manager): https://github.com/astral-sh/uv\n- Bun (JavaScript runtime): https://bun.sh/\n- GitHub CLI: https://cli.github.com/\n\n**Libraries**\n- python-dotenv: https://pypi.org/project/python-dotenv/\n- Starlette (FastAPI base): https://www.starlette.io/\n\n### 8.3 Related Projects\n\n**Similar Open-Source Projects**\n- Text-to-SQL: https://github.com/sqlchat/sqlchat\n- DataChat: https://github.com/datachat/datachat\n- Vanna.AI: https://github.com/vanna-ai/vanna\n\n**ADW System**\n- Internal automation framework for GitHub issue processing\n- Located in `adws/` directory\n- Uses Claude Code CLI for autonomous implementation\n- Separate from main application\n\n---\n\n## Appendix A: Directory Structure\n\n```\n.\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 client/                 # Frontend (Vite + TypeScript)\n\u2502   \u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.py         # Entry point (CORRECTED: should be main.ts)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_client.py   # API client\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow/       # UI workflows\n\u2502   \u2502   \u251c\u2500\u2500 data/               # Sample datasets\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 uv.lock\n\u2502   \u2502   \u2514\u2500\u2500 run.sh\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 server/                 # Backend (FastAPI)\n\u2502       \u251c\u2500\u2500 core/\n\u2502       \u2502   \u251c\u2500\u2500 file_processor.py    # CSV/JSON parsing\n\u2502       \u2502   \u251c\u2500\u2500 llm_processor.py     # LLM integration\n\u2502       \u2502   \u251c\u2500\u2500 sql_processor.py     # SQL execution\n\u2502       \u2502   \u251c\u2500\u2500 sql_security.py      # Security module\n\u2502       \u2502   \u251c\u2500\u2500 insights.py          # Column insights\n\u2502       \u2502   \u251c\u2500\u2500 csv_exporter.py      # Export functionality\n\u2502       \u2502   \u251c\u2500\u2500 data_models.py       # Pydantic models\n\u2502       \u2502   \u2514\u2500\u2500 constants.py         # Config constants\n\u2502       \u251c\u2500\u2500 tests/\n\u2502       \u2502   \u251c\u2500\u2500 core/\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 test_file_processor.py\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 test_llm_processor.py\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 test_sql_processor.py\n\u2502       \u2502   \u251c\u2500\u2500 assets/          # Test fixtures\n\u2502       \u2502   \u2514\u2500\u2500 test_sql_injection.py\n\u2502       \u251c\u2500\u2500 server.py            # FastAPI app\n\u2502       \u251c\u2500\u2500 main.py              # Entry point\n\u2502       \u251c\u2500\u2500 pyproject.toml       # Dependencies\n\u2502       \u251c\u2500\u2500 uv.lock\n\u2502       \u2514\u2500\u2500 .env.sample\n\u2502\n\u251c\u2500\u2500 adws/                        # ADW automation system\n\u2502   \u251c\u2500\u2500 adw_plan_build.py\n\u2502   \u251c\u2500\u2500 trigger_cron.py\n\u2502   \u251c\u2500\u2500 trigger_webhook.py\n\u2502   \u2514\u2500\u2500 README.md\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 start.sh                 # Start both services\n\u2502   \u251c\u2500\u2500 stop_apps.sh             # Stop all services\n\u2502   \u251c\u2500\u2500 reset_db.sh              # Reset database\n\u2502   \u2514\u2500\u2500 copy_dot_env.sh\n\u2502\n\u251c\u2500\u2500 specs/                       # Feature specifications\n\u251c\u2500\u2500 app_docs/                    # Feature documentation\n\u251c\u2500\u2500 agents/                      # Agent execution logs\n\u251c\u2500\u2500 logs/                        # Structured logs\n\u2514\u2500\u2500 README.md                    # Project README\n```\n\n## Appendix B: Environment Variables\n\n```bash\n# .env.sample (app/server/)\n\n# LLM Provider (choose one or both)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n\n# LLM Configuration\nLLM_PROVIDER=openai              # or 'anthropic'\nLLM_MODEL=gpt-4                  # or 'claude-3-5-sonnet-20241022'\nLLM_TEMPERATURE=0.0              # Lower = more deterministic\n\n# Database\nDATABASE_PATH=./db/app.db        # or ':memory:' for in-memory\nDATABASE_BACKUP_PATH=./db/backup.db\n\n# Server\nHOST=0.0.0.0\nPORT=8000\nDEBUG=true\n\n# CORS\nCORS_ORIGINS=http://localhost:5173,http://localhost:3000\n\n# File Upload\nMAX_FILE_SIZE_MB=50\nALLOWED_EXTENSIONS=.csv,.json,.jsonl\n\n# Logging\nLOG_LEVEL=INFO\nLOG_FILE=./logs/app.log\n```\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: 2025-01-20  \n**Author**: AI Specification Generator  \n**Status**: Ready for Implementation",
    "session_id": "96c60ad3-afa6-42dc-8547-39a337fbaaca",
    "total_cost_usd": 0.24233575000000002,
    "usage": {
      "input_tokens": 2,
      "cache_creation_input_tokens": 33765,
      "cache_read_input_tokens": 0,
      "output_tokens": 7228,
      "server_tool_use": {
        "web_search_requests": 0,
        "web_fetch_requests": 0
      },
      "service_tier": "standard",
      "cache_creation": {
        "ephemeral_1h_input_tokens": 0,
        "ephemeral_5m_input_tokens": 33765
      }
    },
    "modelUsage": {
      "claude-haiku-4-5-20251001": {
        "inputTokens": 5496,
        "outputTokens": 359,
        "cacheReadInputTokens": 0,
        "cacheCreationInputTokens": 0,
        "webSearchRequests": 0,
        "costUSD": 0.007291,
        "contextWindow": 200000
      },
      "claude-sonnet-4-5-20250929": {
        "inputTokens": 2,
        "outputTokens": 7228,
        "cacheReadInputTokens": 0,
        "cacheCreationInputTokens": 33765,
        "webSearchRequests": 0,
        "costUSD": 0.23504475000000002,
        "contextWindow": 200000
      }
    },
    "permission_denials": [],
    "uuid": "8ee3662d-8422-425a-bee0-16e5f7d07c52"
  }
]